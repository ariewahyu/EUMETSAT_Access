{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<img src='./img/DataStore_EUMETSAT.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 EUMETSAT <br>\n",
    "License: MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"./index.ipynb\">← Index</a>\n",
    "<br>\n",
    "<a href=\"./3_Data_Tailor_standalone.ipynb\">← Using the EUMETSAT Data Tailor Standalone with EUMDAC</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The EUMDAC Cookbook\n",
    "\n",
    "Welcome to the EUMDAC Cookbook, a curated collection of practical code examples designed to help users navigate the complexities of accessing and processing data from EUMETSAT's Data Access Services using the EUMDAC Python Library. This cookbook aims to empower researchers, developers, and analysts by providing them with a comprehensive guide to effectively utilize the EUMDAC library for their data needs.\n",
    "\n",
    "## How to Use This Cookbook\n",
    "Each recipe in this cookbook is designed to be self-contained, providing you with the necessary context, code, and explanations to understand and implement the solution effectively. You can navigate through them to find specific solutions or browse through the recipes to familiarize yourself with the library's capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "0. [Setup](#0.-Setup)\n",
    "1. [Download products by daily time range](#1.-Download-products-by-daily-time-range)\n",
    "    - [Adaption: Download only the first product of a defined hour each day](#1.1-Adaption:-Download-only-the-first-product-of-a-defined-hour-each-day)\n",
    "2. [Verify downloaded products from Data Store](#2.-Verify-downloaded-products-from-Data-Store)\n",
    "3. [Store original product names while customising the product](#3.-Store-original-product-names-while-customising-the-product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "All our recipes need some general packages and need to authenticate to our Data Access Services APIs via user's personal consumer key and consumer secret. More info about the authentication can be found in the [Discovering Collections](./1_Discovering_collections.ipynb#Authentication) notebook within this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eumdac # library for the EUMETSAT Data Access Services\n",
    "import datetime # to ease handling date formats\n",
    "import shutil # used for downloading products to local disk\n",
    "\n",
    "# Generate an access token with your personal Consumer Key and Consumer Secret\n",
    "token = eumdac.AccessToken(('INSERT_CONSUMER_KEY_HERE', 'INSERT_CONSUMER_SECRET_HERE'))\n",
    "\n",
    "# Access token is used to authenticate with our Data Store APIs\n",
    "datastore = eumdac.DataStore(token)\n",
    "\n",
    "# Access token is used to authenticate with our Data Tailor APIs\n",
    "datatailor = eumdac.DataTailor(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download products by daily time range\n",
    "\n",
    "The script below searches for all HRSEVIRI products between two dates, but only downloads the products from defined hours, e.g. products from 08:00, 10:00 and 12:00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the collection ID you want to filter products in\n",
    "collectionID = 'EO:EUM:DAT:MSG:HRSEVIRI'\n",
    "selected_collection = datastore.get_collection(collectionID)\n",
    "\n",
    "# Define the search parameters\n",
    "start = datetime.datetime(2024, 1, 1) # Start date for product search\n",
    "end = datetime.datetime(2024, 1, 3) # End date for product search\n",
    "delta = datetime.timedelta(days=1) # Define steps to check between the start and end date, e.g. days=1 for check every day, hours=1 for every hour, weeks=2 for bi-weekly\n",
    "hours = ['08','10','12'] # Define the hours per day you want to download products from, e.g. ['10','11'] to download all products of a day sensed between 10 and 12 AM.\n",
    "\n",
    "total_products = 0\n",
    "\n",
    "# Go through every day between the defined start and end date\n",
    "for i in range((end - start).days):\n",
    "    for hour in hours:\n",
    "        # For every hour, defined in `hours`, we are doing a product search request to \n",
    "        # the Data Store OpenSearch API and using the `title` filter and filename pattern\n",
    "        # to search for the desired daily time windows.\n",
    "        date = start + i*delta\n",
    "        products = selected_collection.search(\n",
    "            dtstart=start,\n",
    "            dtend=end,\n",
    "            title=f\"*-{(date).strftime('%Y%m%d')}{hour}****.*\")\n",
    "        \n",
    "        total_products = total_products + products.total_results\n",
    "        \n",
    "        print(f\"{products.total_results} products available for reguested daily time window ({hour}) on {date.day}/{date.month}/{date.year}.\")\n",
    "        \n",
    "        # Download the found products\n",
    "        for product in products:\n",
    "            print(f\"Start to download product {product} from {product.sensing_end}.\")\n",
    "            with product.open() as fsrc, \\\n",
    "                    open(fsrc.name, mode='wb') as fdst:\n",
    "                shutil.copyfileobj(fsrc, fdst)\n",
    "            print(f'Download of product {product} finished.')\n",
    "        \n",
    "print(f\"{total_products} products were downloaded for requested daily time window ({', '.join(hours)}) from {start.day}/{start.month}/{start.year} to {end.day}/{end.month}/{end.year}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Adaption: Download only the first product of a defined hour each day\n",
    "\n",
    "The script below searches for all HRSEVIRI products sensed in January 2024, but downloads only the first product, starting from 12:00 noon of each day. Meaning, this example downloads only one product per day. Additional hours can be added to `hours` for downloading the first product of the defined hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the collection ID you want to filter products in\n",
    "collectionID = 'EO:EUM:DAT:MSG:HRSEVIRI'\n",
    "selected_collection = datastore.get_collection(collectionID)\n",
    "\n",
    "# Define the search parameters\n",
    "start = datetime.datetime(2024, 1, 1) # Start date for product search\n",
    "end = datetime.datetime(2024, 2, 1) # End date for product search\n",
    "delta = datetime.timedelta(days=1) # Define steps to check between the start and end date, e.g. days=1 for check every day, hours=1 for every hour, weeks=2 for bi-weekly\n",
    "hours = ['12'] # Define the hours per day you want to download products from, e.g. ['10','12','14'] to download the first product from after 10:00, 12:00 and 14:00.\n",
    "\n",
    "total_products = 0\n",
    "\n",
    "# Go through every day between the defined start and end date\n",
    "for i in range((end - start).days):\n",
    "    for hour in hours:\n",
    "        # For every hour, defined in `hours`, we are doing a product search request to \n",
    "        # the Data Store OpenSearch API and using the `title` filter and filename pattern\n",
    "        # to search for all products sensed within 12:00 and sort them from oldest to newest.\n",
    "        # With `.first()` we only got the first file in the list. Due to our sorting, \n",
    "        # it's the oldest one or the first sensed after 12:00.\n",
    "        date = start + i*delta\n",
    "        product = selected_collection.search(\n",
    "            dtstart=start,\n",
    "            dtend=end,\n",
    "            title=f\"*-{(date).strftime('%Y%m%d')}{hour}****.*\",\n",
    "            sort=\"start,time,1\").first()\n",
    "        \n",
    "        total_products = total_products + 1\n",
    "                \n",
    "        # Download the found product\n",
    "        print(f\"Start to download product {product} from {product.sensing_end}.\")\n",
    "        with product.open() as fsrc, \\\n",
    "                open(fsrc.name, mode='wb') as fdst:\n",
    "            shutil.copyfileobj(fsrc, fdst)\n",
    "        print(f'Download of product {product} finished.')\n",
    "        \n",
    "print(f\"{total_products} products were downloaded for requested daily time window ({', '.join(hours)}) from {start} to {end}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Verify downloaded products from Data Store\n",
    "\n",
    "The script below provides a way to verify the downloaded files from Data Store via it's MD5 hash to check whether a downloaded file is complete or corrupted. This will be done by comparing the MD5 hash value of the downloaded file with the expected MD5 hash value provided by the Data Store API for each product.\n",
    "If the check is failing, meaning the MD5 hash values are not equal, it shows that the file is corrupt. This can happen due to  download interruptions or network/connectivity issues, e.g. outages. When this happens, we recommend to download the file again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package to generate MD5 of existing file\n",
    "import hashlib\n",
    "\n",
    "# Define the collection ID you want to filter products in\n",
    "collectionID = 'EO:EUM:DAT:MSG:HRSEVIRI'\n",
    "selected_collection = datastore.get_collection(collectionID)\n",
    "\n",
    "# Define the search parameters\n",
    "start = datetime.datetime(2024, 1, 1) # Start date for product search\n",
    "end = datetime.datetime(2024, 1, 1, 0, 15) # End date for product search\n",
    "\n",
    "products = selected_collection.search(\n",
    "    dtstart=start,\n",
    "    dtend=end)\n",
    "\n",
    "for product in products:\n",
    "    print(f'Start download of product {product}...')\n",
    "    with product.open() as fsrc, \\\n",
    "            open(fsrc.name, mode='wb') as fdst:\n",
    "        shutil.copyfileobj(fsrc, fdst)\n",
    "    print(f'Download of product {product} finished. Checking integrity...')\n",
    "    \n",
    "    expected_md5 = product.md5\n",
    "    actual_md5 = hashlib.md5(open(str(product) + '.zip','rb').read()).hexdigest()\n",
    "    \n",
    "    if expected_md5 == actual_md5:\n",
    "        print(\"MD5 integrity check done successfully.\")\n",
    "    else:\n",
    "        print(f\"File seems to be corrupted. MD5s are not equal.\\nPlease, download product {product} again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Store original product names while customising the product\n",
    "\n",
    "The standard procedure for naming a customized product involves using the customization ID as its default identifier. However, this practice may result in ambiguity, as it can be challenging for users to discern the original product from its customized counterpart. To address this issue, we have developed a script that effectively captures and retains the original product ID, typically the filename of the original product. This script then renames the customized product by incorporating the stored original product ID, thereby enhancing clarity and traceability in the product identification process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "\n",
    "# Define the collection ID you want to filter products in\n",
    "collectionID = 'EO:EUM:DAT:MSG:HRSEVIRI'\n",
    "selected_collection = datastore.get_collection(collectionID)\n",
    "\n",
    "# Define the search parameters\n",
    "start = datetime.datetime(2024, 1, 1) # Start date for product search\n",
    "end = datetime.datetime(2024, 1, 1, 0, 15) # End date for product search\n",
    "\n",
    "products = selected_collection.search(\n",
    "    dtstart=start,\n",
    "    dtend=end)\n",
    "\n",
    "# Defining the chain configuration\n",
    "chain = eumdac.tailor_models.Chain(\n",
    "    product='HRSEVIRI',\n",
    "    format='geotiff',\n",
    "    filter={\"bands\" : [\"channel_3\",\"channel_2\",\"channel_1\"]}\n",
    ")\n",
    "\n",
    "for product in products:\n",
    "    # Save metadata from original product to variables\n",
    "    original_name = str(product)\n",
    "    sensing_start = product.sensing_start\n",
    "    sensing_end = product.sensing_end\n",
    "\n",
    "    # Define final product file name\n",
    "    filename = original_name + \".tif\"\n",
    "\n",
    "    # Start customisation\n",
    "    customisation = datatailor.new_customisation(product, chain)\n",
    "    status = customisation.status\n",
    "    sleep_time = 10 # seconds\n",
    "    \n",
    "    # Customisation Loop\n",
    "    while status:\n",
    "        # Get the status of the ongoing customisation\n",
    "        status = customisation.status\n",
    "    \n",
    "        if \"DONE\" in status:\n",
    "            print(f\"Customisation {customisation._id} is successfully completed.\")\n",
    "            break\n",
    "        elif status in [\"ERROR\",\"FAILED\",\"DELETED\",\"KILLED\",\"INACTIVE\"]:\n",
    "            print(f\"Customisation {customisation._id} was unsuccessful. Customisation log is printed.\\n\")\n",
    "            print(customisation.logfile)\n",
    "            break\n",
    "        elif \"QUEUED\" in status:\n",
    "            print(f\"Customisation {customisation._id} is queued.\")\n",
    "        elif \"RUNNING\" in status:\n",
    "            print(f\"Customisation {customisation._id} is running.\")\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "\n",
    "    # Download customised product\n",
    "    jobID= customisation._id\n",
    "    tif, = fnmatch.filter(customisation.outputs, '*.tif')\n",
    "    print(f\"Dowloading the TIF output of the customisation {jobID}\")\n",
    "    \n",
    "    try:\n",
    "        with customisation.stream_output(tif,) as stream, \\\n",
    "                open(filename, mode='wb') as fdst:\n",
    "            shutil.copyfileobj(stream, fdst)\n",
    "        print(f\"Dowloaded successfully the TIF output of the customisation {jobID}: {fdst.name}\")\n",
    "    except requests.exceptions.RequestException as error:\n",
    "        print(f\"Unexpected error: {error}\")\n",
    "\n",
    "    # Delete customisation when it's downloaded\n",
    "    try:\n",
    "        customisation.delete()\n",
    "    except requests.exceptions.RequestException as error:\n",
    "        print(\"Unexpected error:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"./index.ipynb\">← Index</a>\n",
    "<br>\n",
    "<a href=\"./3_Data_Tailor_standalone.ipynb\">← Using the EUMETSAT Data Tailor Standalone with EUMDAC</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<p style=\"text-align:left;\">This project is licensed under the <a href=\"./LICENSE.txt\">MIT License</a> | <span style=\"float:right;\"><a href=\"https://gitlab.eumetsat.int/eumetlab/data-services/\">View on GitLab</a> | <a href=\"https://training.eumetsat.int/\">EUMETSAT Training</a> | <a href=mailto:ops@eumetsat.int>Contact</a></span></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "f61363903226f946a13497630069b9119a23165c9161346d2c3a6f0af415279b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
